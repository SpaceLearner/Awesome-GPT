# Awesome-GPT [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Awesome papers, datasets and projects about the study of large language models like GPT-3, GPT-3.5, ChatGPT, GPT-4, etc.

# Papers

## Survey

* A Survey on In-context Learning (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2301.00234.pdf)]

* A SURVEY ON GPT-3 (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2212.00857.pdf)]

* A Survey of Large Language Models (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2303.18223.pdf)]

## 2023

* GPT-4 Technical Report (**OPENAI, 2023**) [[paper](https://cdn.openai.com/papers/gpt-4.pdf)]

* ReAct: Synergizing Reasoning and Acting in Language Models (**ICLR, 2023, Notable-top-5%**) [[paper](https://openreview.net/pdf?id=WE_vluYUL-X)][[code](https://anonymous.4open.science/r/ReAct-2268/)]

* Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning (**ICLR, 2023, Notable-top-5%**) [[paper](https://openreview.net/pdf?id=3Pf3Wg6o-A4)]

* What learning algorithm is in-context learning? Investigations with linear models (**ICLR, 2023, Notable-top-5%**) [[paper](https://openreview.net/pdf?id=0g0X4H8yN4I)]

* Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. (**ICLR, 2023**) [[paper](https://arxiv.org/pdf/2210.01240.pdf)][[code](https://github.com/asaparov/prontoqa)]

* Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2303.04671.pdf)][[code](https://github.com/microsoft/visual-chatgpt)]

* Toolformer: Language Models Can Teach Themselves to Use Tools. (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2302.04761.pdf)][[code](https://github.com/lucidrains/toolformer-pytorch)]

* Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2302.12813.pdf)]

* Can GPT-3 Perform Statutory Reasoning? (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2302.06100v1.pdf)]

* How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2301.07597.pdf)]

* Large Language Models Can Be Easily Distracted by Irrelevant Context (**ARXIX, 2023**) [[paper](https://arxiv.org/pdf/2302.00093.pdf)]

* Theory of Mind May Have Spontaneously Emerged in Large Language Models (**ARXIV, 2023**) [[paper](https://arxiv.org/ftp/arxiv/papers/2302/2302.02083.pdf)]

* ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2212.14882.pdf)]

## 2022

* Large Language Models are Zero-Shot Reasoners (**Neurips, 2022**) [[paper](https://arxiv.org/pdf/2205.11916.pdf)]

* Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [[paper](https://arxiv.org/pdf/2201.11903.pdf)]

* Automatic Chain of Thought Prompting in Large Language Models [[paper](https://arxiv.org/pdf/2210.03493.pdf)]

* What can transformers learn in-context? a case study of simple function classes (**ARXIV, 2022**) [[paper](https://arxiv.org/pdf/2208.01066.pdf)][[code](https://github.com/dtsip/in-context-learning)]

* Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? (**ARXIV, 2022**) [[paper](https://arxiv.org/pdf/2202.12837.pdf)][[code](https://github.com/Alrope123/rethinking-demonstrations)]

* Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers (**ARXIV, 2022**) [[paper](https://arxiv.org/pdf/2212.10559.pdf)]

## 2021

* Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization (**PMLR, 2021**) [[paper](https://proceedings.mlr.press/v149/chintagunta21a/chintagunta21a.pdf)]

* GPT Understands, Too (**ARXIV, 2021**) [[paper](https://arxiv.org/pdf/2103.10385.pdf)]

* WebGPT: Browser-assisted question-answering with human feedback （**ARXIV, 2021**） [[paper](https://arxiv.org/pdf/2112.09332.pdf)][[code](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api)]

## 2020

* Language Models are Few-shot Learners (**Neurips, 2020**) [[paper](https://arxiv.org/pdf/2005.14165.pdf)]

# Datasets

* [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)

# Projects

* [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)

# Awesome-GPT [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Awesome papers, datasets and projects about the study of large language models like GPT-3, GPT-3.5, ChatGPT, GPT-4, etc.

# Papers

## Survey

* A Survey on In-context Learning (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2301.00234.pdf)]

## 2023

* ReAct: Synergizing Reasoning and Acting in Language Models (**ICLR, 2023, Notable-top-5%**) [[paper](https://openreview.net/pdf?id=WE_vluYUL-X)][[code](https://anonymous.4open.science/r/ReAct-2268/)]

* Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning (**ICLR, 2023, Notable-top-5%**) [[paper](https://openreview.net/pdf?id=3Pf3Wg6o-A4)]

* What learning algorithm is in-context learning? Investigations with linear models (**ICLR, 2023, Notable-top-5%**) [[paper](https://openreview.net/pdf?id=0g0X4H8yN4I)]

* Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought (**ICLR, 2023**) [[paper](https://arxiv.org/pdf/2210.01240.pdf)][[code](https://github.com/asaparov/prontoqa)]

* How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2301.07597.pdf)]

* Large Language Models Can Be Easily Distracted by Irrelevant Context (**ARXIX, 2023**) [[paper](https://arxiv.org/pdf/2302.00093.pdf)]

* Theory of Mind May Have Spontaneously Emerged in Large Language Models (**ARXIV, 2023**) [[paper](https://arxiv.org/ftp/arxiv/papers/2302/2302.02083.pdf)]

* ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports (**ARXIV, 2023**) [[paper](https://arxiv.org/pdf/2212.14882.pdf)]

## 2022

* Large Language Models are Zero-Shot Reasoners (**Neurips, 2022**) [[paper](https://arxiv.org/pdf/2205.11916.pdf)]

* What can transformers learn in-context? a case study of simple function classes (**ARXIV, 2022**) [[paper](What Can Transformers Learn In-Context?
A Case Study of Simple Function Classes)][[code](https://github.com/dtsip/in-context-learning)]

* Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? (**ARXIV, 2022**) [[paper](https://arxiv.org/pdf/2202.12837.pdf)]

* Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers (**ARXIV, 2022**) [[paper](https://arxiv.org/pdf/2212.10559.pdf)]

## 2021

* Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization (**PMLR, 2021**) [[paper](https://proceedings.mlr.press/v149/chintagunta21a/chintagunta21a.pdf)]

* GPT Understands, Too (**ARXIV, 2021**) [[paper](https://arxiv.org/pdf/2103.10385.pdf)]

## 2020

* Language Models are Few-shot Learners (**Neurips, 2020**) [[paper](https://arxiv.org/pdf/2005.14165.pdf)]

# Datasets

* [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)

# Projects

* [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)
